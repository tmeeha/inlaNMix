\documentclass[codesnippet]{jss}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{hyperref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Timothy D. Meehan\\National\\Audubon Society \And
	Nicole L. Michel\\National\\Audubon Society \And
	H{\aa}vard Rue\\King Abdulla University\\of Science and Technology}
\title{Estimating Animal Abundance with N-Mixture Models Using the \pkg{R-INLA} Package for R}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Timothy D. Meehan, Nicole L. Michel, Haavard Rue} %% comma-separated
\Plaintitle{Estimating Animal Abundance with N-Mixture Models Using the R-INLA Package for R} %% without formatting
\Shorttitle{\pkg{R-INLA} for N-Mixture Models}

%% an abstract and keywords
\Abstract{
  Successful management of wildlife populations requires accurate estimates of abundance. Abundance estimates can be confounded by imperfect detection during wildlife surveys. N-mixture models enable quantification of detection probability and, under appropriate conditions, produce abundance estimates that are less biased. Here we demonstrate use of the \pkg{R-INLA} package for \proglang{R} to analyze N-mixture models and to compare performance of \pkg{R-INLA} to two other common approaches: \proglang{JAGS} (via the \pkg{runjags} package for \proglang{R}), which uses Markov chain Monte Carlo and allows Bayesian inference, and the \pkg{unmarked} package for \proglang{R}, which uses maximum likelihood and allows frequentist inference. We show that \pkg{R-INLA} is an attractive option for analyzing N-mixture models when (1) fast computing times are necessary (\pkg{R-INLA} is 10 times faster than \pkg{unmarked} and 500 times faster than \proglang{JAGS}), (2) familiar model syntax and data format (relative to other \proglang{R} packages) are desired, (3) survey-level covariates of detection are not essential, and (4) Bayesian inference is preferred.
}
\Keywords{abundance, detection, \proglang{JAGS}, N-mixture model, \proglang{R}, \pkg{R-INLA}, \pkg{unmarked}, wildlife}
%% without formatting
\Plainkeywords{wildlife, abundance, detection, N-mixture models, R-INLA, R, BUGS} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
Timothy D. Meehan\\
National Audubon Society\\
Boulder, Colorado USA\\
E-mail: \email{tmeehan@audubon.org}\\
\\
Nicole L. Michel\\
National Audubon Society\\
San Francisco, California USA\\
E-mail: \email{nmichel@audubon.org}\\
\\
H{\aa}vard Rue\\
King Abdulla University
of Science and Technology\\
Thuwal, Saudi Arabia\\
E-mail: \email{haavard.rue@kaust.edu.sa }
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% start main document %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
	
%% introduction
\section[Introduction]{Introduction}
\subsection[Background]{Background}
Successful management of wildlife species requires accurate estimates of abundance \citep{Yoccoz_Nichols_Boulinier_2001}. One common method for estimating animal abundance is direct counts \citep{Pollock_Nichols_Simons_Farnsworth_Bailey_Sauer_2002}. Efforts to obtain accurate abundance estimates via direct counts can be hindered by the cryptic nature of many wildlife species, and by other factors such as observer expertise, weather, and habitat structure \citep{Denes_Silveira_Beissinger_2015}. The lack of perfect detection in wildlife surveys is common, and can cause abundance to be underestimated \citep{Wenger_Freeman_2008, Joseph_Elkin_Martin_Possingham_2009}.

In recent years, new sampling schemes and modeling approaches have enabled improved estimates of animal abundance that are less biased by imperfect detection \citep{Denes_Silveira_Beissinger_2015}. One such sampling scheme, termed a metapopulation design \citep{Kery_Royle_2010}, involves repeat visits in rapid succession to each of multiple study sites in a study area. If, during repeat visits, the population is assumed to be closed (no immigration, emigration, reproduction or mortality; i.e., static abundance), then information on detections and non-detections during repeated counts can inform an estimate of detection probability. This detection probability can be used to correct abundance estimates for imperfect detection \citep{Royle_2004}.

Data resulting from this sampling scheme are often modeled using an explicitly hierarchical statistical model referred to in the quantitative wildlife ecology literature as an N-mixture model \citep{Royle_Nichols_2003, Dodd_Dorazio_2004, Royle_2004, Kery_Royle_Schmid_2005}. A simple form of an N-mixture model, a binomial mixture model, describes individual observed counts $y$ at site $i$ during survey $j$ as coming from a binomial distribution with parameters for abundance $N$ and detection probability $p$, where $N$ per site is drawn from a Poisson distribution with an expected value $\lambda$. Specifically,

$$N_i \sim \text{Pois}(\lambda) \qquad \text{and} \qquad  y_{i,j} | N_i \sim \text{Bin}(N_i, p).$$

$\lambda$ is commonly modeled as a log-linear function of site covariates, as $\text{log}(\lambda_i) = \beta_0 + \beta_1 x_i$. Similarly, $p$ is commonly modeled as $\text{logit}(p_{i,j}) = \alpha_0 + \alpha_1 x_{i,j}$, a logit-linear function of site-survey covariates.

This estimation approach can be extended to cover $K$ distinct breeding or wintering seasons, which correspond with distinct years for wildlife species that are resident during annual breeding or wintering stages \citep{Kery_Dorazio_Soldaat_Van_Strien_Zuiderwijk_Royle_2009}. In this case, population closure is assumed across $J$ surveys within year $k$, but is relaxed across years \citep{Kery_Dorazio_Soldaat_Van_Strien_Zuiderwijk_Royle_2009}. A simple specification of a multiple-year model is $N_{i,k} \sim \text{Pois}(\lambda_{k}), \ y_{i,j,k} | N_{i,k} \sim \text{Bin}(N_{i,k}, p_{k})$. Like the single-year specification, $\lambda$ is commonly modeled using site and site-year covariates, and $p$ using site-survey-year covariates. 

There are other variations of N-mixture models that accommodate overdispersed counts through use of a negative binomial distribution \citep{Kery_Royle_2010}, a zero-inflated Poisson distribution \citep{Wenger_Freeman_2008}, or survey-level random intercepts \citep{Kery_Schaub_2011}, or underdispersed counts using mixtures of binomial and Conway-Maxwell-Poisson distributions \citep{wu2015bayesian}. Yet other variations account for non-independent detection probabilities through use of a beta-binomial distribution \citep{Martin_Royle_Mackenzie_Edwards_Kery_Gardner_2011}, parse different components of detection through the use of unique covariates \citep{O'Donnell_Thompson_III_Semlitsch_2015}, or relax assumptions of population closure \citep{Chandler_Royle_King_2011, Dail_Madsen_2011}. We do not discuss all of these variations here, but refer interested readers to \cite{Denes_Silveira_Beissinger_2015} for an overview, and to \cite{Barker_Schofield_Link_Sauer_et_al_2017} for a discussion of assumptions and limitations.

The development of metapopulation designs and N-mixture models represents a significant advance in quantitative wildlife ecology. However, there are practical issues that sometimes act as barriers to adoption. Many of the examples of N-mixture models in the wildlife literature have employed Bayesian modeling software such as \proglang{WinBUGS}, \proglang{OpenBUGS}, \proglang{JAGS}, or \proglang{Stan} \citep{plummer2003jags,Lunn_Jackson_Best_Thomas_Spiegelhalter_2012,Carpenter_Gelman_Hoffman_Lee_et_al_2017}. These are extremely powerful and flexible platforms for analyzing hierarchical models, but they come with a few important challenges. First, many wildlife biologists are not accustomed to coding statistical models using the \proglang{BUGS} or \proglang{Stan} modeling syntax. While there are several outstanding resources aimed at teaching these skills \citep{Royle_Dorazio_2008, Kery_2010, Kery_Schaub_2011, Kery_Royle_2015, Korner-Nievergelt_Roth_et_al_2015} learning them is, nonetheless, a considerable commitment. Second, while Markov chain Monte Carlo (MCMC) chains converge quickly for relatively simple N-mixture models, convergence for more complex models can take hours to days, or may not occur at all \citep{Kery_Schaub_2011}.

There are other tools available for analyzing N-mixture models that alleviate some of these practical issues. The \pkg{unmarked} package \citep{Fiske_Chandler_2011} for \proglang{R} statistical computing software \citep{R_Core_Team_2016} offers several options for analyzing N-mixture models within a maximum likelihood (ML) framework, with the capacity to accommodate overdispersed counts and dynamic populations. The model coding syntax used in \pkg{unmarked} is a simple extension of the standard \proglang{R} modeling syntax. Models are analyzed using ML, so model analysis is often completed in a fraction of the time taken using MCMC. The familiar model syntax and rapid model evaluation of \pkg{unmarked} has undoubtedly contributed to the broader adoption of N-mixture models by wildlife biologists. However, it comes at a cost, loss of the intuitive inferential framework associated with Bayesian analysis.

Here we demonstrate analysis of N-mixture models using the \pkg{R-INLA} package \citep{Martins_Simpson_Lindgren_Rue_2013,Rue_Riebler_Sorbye_Illian_Simpson_Lindgren_2017} for \proglang{R}. The \pkg{R-INLA} package uses integrated nested Laplace approximation (INLA) to derive posterior distributions for a large class of Bayesian statistical models that can be formulated as latent Gaussian models \citep{Rue_Martino_Chopin_2009, Lindgren_Rue_Lindstrom_2011}. INLA was developed to allow estimation of posterior distributions in a fraction of the time taken by MCMC. Like \pkg{unmarked}, the model syntax used by the \pkg{R-INLA} package is a straightforward extension of the modeling syntax commonly used in \proglang{R}. Also, like \pkg{unmarked}, the computational cost of analyzing models with \pkg{R-INLA} is relatively low compared to MCMC. The \pkg{R-INLA} approach is different from \pkg{unmarked} in that inference about model parameters falls within a Bayesian framework.

\subsection[Overall objectives]{Overall objectives}
The purpose of this manuscript is to present a comparative analysis of N-mixture models that is centered on the \pkg{R-INLA} package. In the process, we employ both simulated and real count datasets, and analyze them using \pkg{R-INLA}, \proglang{JAGS}, via the \pkg{runjags} package \citep{Denwood_2016} for \proglang{R}, and the \pkg{unmarked} package for \proglang{R}. In each case, we demonstrate how models are specified, how model estimates compare to simulation inputs and to each other, and how methods compare in terms of computational performance. When describing \pkg{R-INLA} analyses, we detail the format of input data and the content of analysis code, to facilitate users conducting their own analyses.

We also explore a limitation of the \pkg{R-INLA} approach related to model specification. In particular, while it is possible to specify survey-level covariates for detection using \proglang{JAGS} and \pkg{unmarked}, this is not possible using \pkg{R-INLA}. Rather, survey-level covariates of detection must be averaged to the site or site-year level. Using an averaged detection covariate does allow accounting for site or site-year differences in survey conditions, should they occur. However, in the process of averaging, information related to detection within a site or site-year combination is discarded, which could lead to biased detection and abundance estimates under certain conditions.

Much of the code used to conduct the \pkg{R-INLA} analyses is shown in the body of this manuscript. However, some repeated \pkg{R-INLA} code, code used in \proglang{JAGS} and \pkg{unmarked} analyses, and code related to generating figures, is not shown, for brevity. All code, fully commented, can be accessed via \url{https://github.com/tmeeha/inlaNMix}. Regarding code, note that the \pkg{R-INLA} package is atypical among R packages in a few different ways.  First, \pkg{R-INLA} is not available on CRAN, as are many other \proglang{R} packages. Second, \pkg{R-INLA} was initially called \pkg{INLA}, based on its origin as a stand-alone \proglang{C} program.  Over time, community reference to the packaged evolved to become \pkg{R-INLA}.  However, installing and loading the package still employs the original name, which may cause some confusion. To install the package, paste \code{install.packages("INLA", repos="https://inla.r-inla-download.org/R/stable")} \\into an \proglang{R} console. To load the package, use the \proglang{R} command \code{library(INLA)}. See \url{https://r-inla.org} to connect with the community around the development of \pkg{R-INLA} and its application to geostatistics, biostatistics, epidemiology, and econometrics \citep{Lindgren_Rue_2015,Blangiardo_Cameletti_2015}.

\section[Example data]{Example data}
\subsection[Simulated data]{Simulated data}
The data simulated for Example I (Section 3) and Example II (Section 4) were intended to represent a typical wildlife abundance study. To put the simulation into context, consider an effort to estimate the abundance of a bird species in a national park, within which are located 72 study sites. At each site, 3 replicate surveys are conducted within 6 weeks, during the peak of the breeding season, when birds are most likely to be singing. In order to estimate a trend in abundance over time, clusters of repeated surveys are conducted each breeding season over a 9-year period.

In this scenario, the abundance of the species is thought to vary with two site-level covariates (\textit{x1} and \textit{x2}) that represent habitat characteristics at a site and do not change appreciably over time, and a third covariate that indicates the year (\textit{x3}). The detection probability is also believed to vary according to two covariates (\textit{x1} and \textit{x4}). The first covariate for detection, \textit{x1}, is the same site-level \textit{x1} that affects abundance, although it has the opposite effect on detection. The other detection covariate, \textit{x4}, is a site-survey-year variable that could be related to weather conditions during an individual survey. As is common, due to effects of unknown variables, simulated counts were overdispersed. Overdispersed counts were generated and modeled using a negative binomial distribution. Simulation data was generated using the model

$$N_{i,k} \sim \text{NegBin}(\lambda_{i,k}, \theta) \qquad \text{and} \qquad  y_{i,j,k} | N_{i,k} \sim \text{Bin}(N_{i,k}, p_{i,j,k}),$$

where $\lambda_{i,k}$ was a log-linear function of site and year covariates, as $\text{log}(\lambda_{i,k}) = \beta_0 + \beta_1 (\textit{x1}_{i}) + \beta_2 (\textit{x2}_{i}) + \beta_3 (\textit{x3}_{k})$. $p$ was a logit-linear function of site and site-survey-year covariates, as $\text{logit}(p_{i,j,k}) = \alpha_0 + \alpha_1 (\textit{x1}_{i}) + \alpha_4 (\textit{x4}_{i,j,k})$.

Parameter values for the linear predictor for $\lambda$ were set to $\beta_0$ = 2.0, $\beta_1$ = 2.0, $\beta_2$ = -3.0, $\beta_3$ = 1.0. The overdispersion parameter was set to $\theta$ = 3.0. Parameter values for the linear predictor for $p$ were: $\alpha_0$ = 1.0, $\alpha_1$ = -2.0, $\alpha_4$ = 1.0. All independent variables in the simulation were centered at zero to reduce computational difficulties and to make model intercepts more easily interpreted.

We simulated data for Examples I and II using the \code{sim.nmix()} function, shown below, with which we encourage readers to experiment. Parameter and variable names in the function code are similar to those given in the model description, above. Note that the function produces two versions of detection covariate \textit{x4} (\code{x4} and \code{x4.m}) and two versions of the count matrix (\code{Y} and \code{Y.m}). Covariate \code{x4} is the same as the site-survey-year variable \textit{x4} described above. It is used to generate \code{Y}, which is used in Example II. Covariate \code{x4.m} is derived from \code{x4}, where values are unique to site and year, but are averaged and duplicated over surveys. It is used to generate \code{Y.m}, which is employed in Example I. Running \code{sim.nmix()} results in a list containing data frames for use with \pkg{R-INLA} and \pkg{unmarked}, and values and vectors for use with \proglang{JAGS}. Before running the function, we install and load libraries and set the seed for the random number generator so that the results are reproducible.

\begin{CodeInput}
R> install.packages(c("runjags", "unmarked"))
R> install.packages("INLA", repos="https://inla.r-inla-download.org/R/stable")
R> library(INLA)
R> library(runjags)
R> library(unmarked)
R> set.seed(12345)
R> sim.nmix <- function(n.sites = 72,    # number of study sites
+    n.surveys = 3,                      # short term replicates
+    n.years = 9,                        # number of years
+    b0 = 2.0,                           # intercept log lambda
+    b1 = 2.0,                           # slope log lambda, x1
+    b2 = -3.0,                          # slope log lambda, x2
+    b3 = 1.0,                           # slope log lambda, x3
+    a0 = 1.0,                           # intercept logit p
+    a1 = -2.0,                          # slope logit p, x1
+    a4 = 1.0,                           # slope logit p, x4
+    th = 3.0                            # overdisperison parameter
+    ){
+
+    # make empty N and Y arrays
+    if(n.years %% 2 == 0) {n.years <- n.years + 1}
+    N.tr <- array(dim = c(n.sites, n.years))
+    Y <- array(dim = c(n.sites, n.surveys, n.years))
+    Y.m <- array(dim = c(n.sites, n.surveys, n.years))
+
+    # create abundance covariate values
+    x1 <- array(as.numeric(scale(runif(n = n.sites, -0.5, 0.5), scale = F)),
+      dim = c(n.sites, n.years))
+    x2 <- array(as.numeric(scale(runif(n = n.sites, -0.5, 0.5), scale = F)),
+      dim = c(n.sites, n.years))
+    yrs <- 1:n.years; yrs <- (yrs - mean(yrs)) / (max(yrs - mean(yrs))) / 2
+    x3 <- array(rep(yrs, each = n.sites), dim = c(n.sites, n.years))
+
+    # fill true N array
+    lam.tr <- exp(b0 + b1 * x1 + b2 * x2 + b3 * x3)
+    for(i in 1:n.sites){
+      for(k in 1:n.years){
+        N.tr[i, k] <- rnbinom(n = 1, mu = lam.tr[i, k], size = th)
+    }}
+
+    # create detection covariate values
+    x1.p <- array(x1[,1], dim = c(n.sites, n.surveys, n.years))
+    x4 <- array(as.numeric(scale(runif(n = n.sites * n.surveys * n.years,
+      -0.5, 0.5), scale = F)), dim = c(n.sites, n.surveys, n.years))
+
+    # average x4 per site-year for example 1
+    x4.m <- apply(x4, c(1, 3), mean, na.rm = F)
+    out1 <- c()
+    for(k in 1:n.years){
+      chunk1 <- x4.m[ , k]
+      chunk2 <- rep(chunk1, n.surveys)
+      out1 <- c(out1, chunk2)
+    }
+    x4.m.arr <- array(out1, dim = c(n.sites, n.surveys, n.years))
+
+    # fill Y.m count array using x4.m for example 1
+    p.tr1 <- plogis(a0 + a1 * x1.p + a4 * x4.m.arr)
+    for (i in 1:n.sites){
+      for (k in 1:n.years){
+        for (j in 1:n.surveys){
+          Y.m[i, j, k] <- rbinom(1, size = N.tr[i, k], prob = p.tr1[i, j, k])
+    }}}
+
+    # fill Y count array using x4 for example 2
+    p.tr2 <- plogis(a0 + a1 * x1.p + a4 * x4)
+    for (i in 1:n.sites){
+      for (k in 1:n.years){
+        for (j in 1:n.surveys){
+          Y[i, j, k] <- rbinom(1, size = N.tr[i, k], prob = p.tr2[i, j, k])
+    }}}
+
+    # format Y.m for data frame output for inla and unmarked
+    Y.m.df <- Y.m[ , , 1]
+    for(i in 2:n.years){
+      y.chunk <- Y.m[ , , i]
+      Y.m.df <- rbind(Y.m.df, y.chunk)
+    }
+
+    # format covariates for data frame output for inla and unmarked
+    x1.df <- rep(x1[ , 1], n.years)
+    x2.df <- rep(x2[ , 1], n.years)
+    x3.df <- rep(x3[1, ], each = n.sites)
+    x1.p.df <- rep(x1.p[ , 1, 1], n.years)
+    x4.df <- c(x4.m)
+
+    # put together data frames for inla and unmarked
+    inla.df <- unmk.df <- data.frame(y1 = Y.m.df[ , 1], y2 = Y.m.df[ , 2],
+    y3 = Y.m.df[ , 3], x1 = x1.df, x2 = x2.df, x3 = x3.df,
+    x1.p = x1.p.df, x4.m = x4.df)
+
+    # return all necessary data for examples 1 and 2
+    return(list(inla.df = inla.df, unmk.df = unmk.df, n.sites = n.sites,
+      n.surveys = n.surveys, n.years = n.years, x1 = x1[ , 1],
+      x2 = x2[ , 1], x3 = x3[1, ], x4 = x4, x4.m = x4.m, x4.m.arr = x4.m.arr,
+      Y = Y, Y.m = Y.m, lam.tr = lam.tr, N.tr = N.tr, x1.p = x1.p[ , 1, 1]
+    ))
+
+  } # end sim.nmix function
R> sim.data <- sim.nmix()
\end{CodeInput}

\subsection[Real data]{Real data}
In addition to simulated data, we also demonstrate the use of \pkg{R-INLA} and \pkg{unmarked} with a real dataset in Example III in Section 5. This dataset comes from a study by \cite{Kery_Royle_Schmid_2005} and is publicly available as part of the \pkg{unmarked} package. The dataset includes mallard duck (\emph{Anas platyrhynchos}) counts, conducted at 239 sites on 2 or 3 occasions during the summer of 2002, as part of a Swiss program that monitors breeding bird abundance (Monitoring H\"{a}ufige Brutv\"{o}gel or Swiss Breeding Bird Survey). In addition to counts, the dataset also includes 2 site-survey covariates related to detection (survey effort and survey date), and 3 site-level covariates related to abundance (route length, route elevation, and forest cover). Full dataset details are given in \cite{Kery_Royle_Schmid_2005}.

%% example 1
\section[Example I]{Example I}
\subsection[Goals]{Goals}
In Example I, we demonstrate the use of \pkg{R-INLA} and compare use and performance to similar analyses using \proglang{JAGS} and \pkg{unmarked}. In this first example, the functional forms of \pkg{R-INLA}, \proglang{JAGS}, and \pkg{unmarked} models match the data generating process. Specifically, we used the covariate \code{x4.m} to generate the count matrix \code{Y.m} and analyzed the data with models that use \code{x4.m} as a covariate. This example was intended to demonstrate the differences and similarities in use, computation time, and estimation results across the three methods when the specified models were the same as the data generating process.

\subsection[Analysis with R-INLA]{Analysis with \pkg{R-INLA}}
We first analyze the simulated data using the \pkg{R-INLA} package. The list returned from the \code{sim.nmix()} function includes an object called \code{inla.df}. This object has the following structure.  

\begin{CodeInput}
R> str(sim.data$inla.df, digits.d = 2)
\end{CodeInput}
\begin{CodeOutput}
'data.frame':	648 obs. of  8 variables:
  $ y1        : int  2 12 25 3 0 3 1 7 2 8 ...
  $ y2        : int  2 22 25 4 1 3 1 11 2 4 ...
  $ y3        : int  4 11 28 2 1 2 0 10 2 3 ...
  $ x1        : num  0.198 0.353 0.238 0.364 -0.066 ...
  $ x2        : num  -0.159 -0.197 -0.484 0.087 0.429 ...
  $ x3        : num  -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 ...
  $ x1.p      : num  0.198 0.353 0.238 0.364 -0.066 ...
  $ x4.m      : num  0.148 -0.07 0.206 -0.261 -0.046 ...
\end{CodeOutput}
\begin{CodeInput}
R> round(head(sim.data$inla.df), 3)
\end{CodeInput}
\begin{CodeOutput}
     y1    y2    y3        x1        x2     x3     x1.p      x4.m
1     2     2     4     0.198    -0.159   -0.5    0.198     0.148
2    12    22    11     0.353    -0.197   -0.5    0.353    -0.070
3    25    25    28     0.238    -0.484   -0.5    0.238     0.206
4     3     4     2     0.364     0.087   -0.5    0.364    -0.261
5     0     1     1    -0.066     0.429   -0.5   -0.066    -0.046
6     3     3     2    -0.356     0.123   -0.5   -0.356    -0.036
\end{CodeOutput}

This data frame representation of the simulated data has 72 sites $\times$ 9 years = 648 rows. Had there only been one year of data, then the data frame would have 72 rows, one per site. The data frame has three columns (\code{y1}, \code{y2}, and \code{y3}) with count data from the count matrix \code{Y.m}, one for each of the three replicate surveys within a given year. Had there been six surveys per year, then there would have been six count columns. The three variables thought to affect abundance are represented in columns 4 through 6. Note that, in this scenario, the first two abundance variables are static across years, so there are 72 unique values in a vector that is stacked 9 times. The third abundance variable, the indicator for year, is a sequence of 9 values, where each value is repeated 72 times. It is centered and scaled in this example. The two variables thought to affect detection probability are represented in columns 7 and 8. The first of these variables has the same values as in column 4, so column 7 is a simple copy of column 4. The second of the two detection variables, shown in column 8, varies per site and year in Example I, so there are 648 unique values in this column. Note that any of the covariates for abundance or detection could have varied by site and year, like \code{x4.m}.

We made small modifications to this data frame to prepare data for analysis with \pkg{R-INLA}. In the code that follows, we use the \code{inla.mdata()} function to create an object called \code{counts.and.count.covs}. The \code{counts.and.count.covs} object is essentially a bundle of information related to the abundance component of the model.  Calling the \code{str()} function shows that this object is an \pkg{R-INLA} list that includes the three count vectors, passed to the function as a matrix, one vector containing the value of 1, which specifies a global intercept for $\lambda$, and three vectors corresponding to the covariates for $\lambda$. Note that the variable names are standardized by \code{inla.mdata()} for computational reasons. 

\begin{CodeInput}
R> inla.data <- sim.data$inla.df
R> y.mat <- as.matrix(inla.data[,c("y1", "y2", "y3")])
R> counts.and.count.covs <- inla.mdata(y.mat, 1, inla.data$x1, 
+    inla.data$x2, inla.data$x3)
R> str(counts.and.count.covs)
\end{CodeInput}
\begin{CodeOutput}
List of 7
  $ Y1: int [1:648] 2 12 25 3 0 3 1 7 2 8 ...
  $ Y2: int [1:648] 2 22 25 4 1 3 1 11 2 4 ...
  $ Y3: int [1:648] 4 11 28 2 1 2 0 10 2 3 ...
  $ X1: num [1:648] 1 1 1 1 1 1 1 1 1 1 ...
  $ X2: num [1:648] 0.1983 0.3532 0.2384 0.3636 -0.0661 ...
  $ X3: num [1:648] -0.1595 -0.1966 -0.4842 0.0865 0.429 ...
  $ X4: num [1:648] -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 ...
- attr(*, "class")= chr "inla.mdata"
\end{CodeOutput}

Analysis of N-mixture models with \pkg{R-INLA} is accomplished with a call to the \code{inla()} function. The first argument in the \code{inla()} call, shown below, is the model formula. On the left side of the formula is the \code{counts.and.count.covs} object, which includes the vectors of counts, the global intercept for $\lambda$, and the covariates related to $\lambda$. On the right side of the formula is a 1, to specify a global intercept for $p$, and the two covariates for $p$. Note that a wide range of random effects (exchangeable, spatially or temporally structured) for $p$ could be added to the right side of the formula using the \code{f()} syntax \citep{Rue_Riebler_Sorbye_Illian_Simpson_Lindgren_2017}.

The second argument to \code{inla()} describes the data, provided here as a list that corresponds with the model formula. Third is the likelihood family, which can take values of \code{"nmix"} for a Poisson-binomial mixture and \code{"nmixnb"} for a negative binomial-binomial mixture. Run the command \code{inla.doc("nmix")} for more information on these likelihood families. The fourth (\code{control.fixed}, for detection parameters) and fifth (\code{control.family}, for abundance and overdispersion parameters) arguments specify the priors for the two model components. Here, the priors for both abundance and detection parameters are vague normal distributions centered at zero with precision equal to 0.01. The prior for the overdispersion parameter is specified as uniform. Note that a wide variety of other prior distributions are available in \pkg{R-INLA}. At the end of the call are arguments to print the progress of model fitting, and to save information that will enable computation of fitted values. Several other characteristics of the analysis can be modified in a call to \code{inla()}, such as whether or not deviance information criterion (DIC), widely applicable information criterion (WAIC), conditional predictive ordinate (CPO), or probability integral transform (PIT) are computed. See \cite{Rue_Riebler_Sorbye_Illian_Simpson_Lindgren_2017} for details.

\begin{CodeInput}
R> out.inla.1 <- inla(counts.and.count.covs ~ 1 + x1.p + x4.m,
+    data = list(counts.and.count.covs = counts.and.count.covs,
+      x1.p = inla.data$x1.p, x4.m = inla.data$x4.m),
+    family = "nmixnb",
+    control.fixed = list(mean = 0, mean.intercept = 0, prec = 0.01,
+      prec.intercept = 0.01),
+    control.family = list(hyper = list(theta1 = list(param = c(0, 0.01)),
+      theta2 = list(param = c(0, 0.01)), theta3 = list(param = c(0, 0.01)),
+      theta4 = list(param = c(0, 0.01)), theta5 = list(prior = "flat",
+      param = numeric()))),
+    verbose = TRUE,
+    control.compute=list(config = TRUE))
R> summary(out.inla.1, digits = 3)
\end{CodeInput}
\begin{CodeOutput}
Time used (seconds):
Pre-processing    Running inla    Post-processing    Total 
0.421             5.081           0.342              5.844 

Fixed effects:
              mean     sd  0.025quant 0.5quant  0.975quant
(Intercept)  1.053  0.058       0.938     1.054      1.165
x1.p        -1.996  0.197      -2.385    -1.995     -1.611
x4.m         1.056  0.313       0.440     1.056      1.668

The model has no random effects.

Model hyperparameters:
             mean      sd  0.025quant  0.5quant 0.975quant
beta[1]     2.022   0.034       1.956     2.022      2.090
beta[2]     2.070   0.116       1.839     2.071      2.295
beta[3]    -2.951   0.099      -3.142    -2.953     -2.755
beta[4]     1.142   0.088       0.969     1.142      1.316
overdisp    0.349   0.028       0.296     0.349      0.407
\end{CodeOutput}

Partial output from the \code{summary()} function, run on the \code{out.inla.1} object, returned from \code{inla()} function, is shown above. The analysis of the model took approximately 6 seconds.  Information on the intercept and covariates related to detection are found under the fixed effects section.  Note that the posterior median parameter estimates related to the detection intercept ($\alpha_0$ labeled as \code{(Intercept)}) and covariates ($\alpha_1$ as \code{x1.p} and $\alpha_4$ as \code{x4.m}) are very close to, and not significantly different from, input parameter values (Fig. \ref{fig:fig1}). Information on abundance and overdispersion parameters are given in the model hyperparameters section. Posterior median estimates for $\beta_0$ (labeled as \code{beta[1]}), $\beta_1$ (\code{beta[2]}), $\beta_2$ (\code{beta[3]}), $\beta_3$ (\code{beta[4]}), and $\theta$ (1 / \code{overdisp} = 2.87) are also very close to, and not significantly different from, input parameter values (Fig. \ref{fig:fig1}). Density plots of the full marginal posterior distributions for model parameters (Fig. \ref{fig:fig1}) can be viewed using \code{plot(out.inla.1)}.

$\lambda_{i,k}$ for each site by year combination in the dataset can be computed using covariate values given in the \code{counts.and.count.covs} object, combined with posterior distributions of parameters in the linear predictor of $\lambda_{i,k}$. Posterior distributions for computed $\lambda_{i,k}$ values can be estimated by repeated sampling from the posteriors of hyperparameters, using the \code{inla.hyperpar.sample()} function, and repeated solving of the linear predictor. The helper function, \code{inla.nmix.lambda.fitted()} produces fitted lambda values as described, using the information contained in the model result output. A call to this function, specifying the model result, estimated posterior sample size, and summary output, is as follows.

\begin{CodeInput}
R> out.inla.1.lambda.fits <- inla.nmix.lambda.fitted(result = out.inla.1, 
+    sample.size = 5000, return.posteriors = FALSE)$fitted.summary
R> head(out.inla.1.lambda.fits)
\end{CodeInput}
\begin{CodeOutput}
  index mean.lambda sd.lambda  q025.lambda  median.lambda   q975.lambda
1     1     10.3329    0.6623       9.0980        10.3109       11.6683
2     2     15.9003    1.1895      13.6760        15.8649       18.3523
3     3     29.2742    2.2882      25.0410        29.2043       34.0370
4     4      7.0490    0.5250       6.0663         7.0270        8.1227
5     5      1.0547    0.0810       0.9072         1.0509        1.2194
6     6      1.4272    0.1031       1.2389         1.4255        1.6353
\end{CodeOutput}

The output from this function call is a summary of estimated posteriors for fitted $\lambda_{i,k}$ values. In this example, there are 648 rows. Comparisons of median fitted $\lambda$ and simulated $\lambda$ and $N$ values are shown below.

\begin{CodeInput}
R> summary(out.inla.1.lambda.fits$median.lambda)
R> summary(c(sim.data$lam.tr))
\end{CodeInput}
\begin{CodeOutput}
  Min.   1st Qu.    Median     Mean    3rd Qu.       Max. 
0.6538    3.4020    7.0070  13.5200    16.0300   123.2000
  Min.   1st Qu.    Median     Mean    3rd Qu.       Max. 
0.6834    3.3050    6.8190  13.0300    15.9500   111.3000 
\end{CodeOutput}

\begin{CodeInput}
R> cor(out.inla.1.lambda.fits$median.lambda, c(sim.data$lam.tr))
\end{CodeInput}
\begin{CodeOutput}
[1] 0.9986
\end{CodeOutput}

\begin{CodeInput}
R> sum(out.inla.1.lambda.fits$median.lambda)
R> sum(c(sim.data$lam.tr))
R> sum(c(sim.data$N.tr))
\end{CodeInput}
\begin{CodeOutput}
[1] 8758.542
[1] 8444.975
[1] 8960
\end{CodeOutput}

\subsection[Analysis with JAGS]{Analysis with \proglang{JAGS}}
Next, we analyzed the same simulated dataset using \proglang{JAGS}, via the \pkg{runjags} package. As for the \pkg{R-INLA} analysis, we specified a negative binomial distribution for abundance, vague normal priors for the intercepts and the global effects of the covariates of $\lambda$ and $p$, and a flat prior for the overdispersion parameter.  The \proglang{JAGS} model statement, where the distributions and likelihood function are specified, is shown below for comparison with the arguments to \code{inla()}.

\begin{CodeInput}
R> jags.model.string <- "
+    model {
+      a0 ~ dnorm(0, 0.01)
+      a1 ~ dnorm(0, 0.01)
+      a4 ~ dnorm(0, 0.01)
+      b0 ~ dnorm(0, 0.01)
+      b1 ~ dnorm(0, 0.01)
+      b2 ~ dnorm(0, 0.01)
+      b3 ~ dnorm(0, 0.01)
+      th ~ dunif(0, 5)
+    for (k in 1:n.years){
+      for (i in 1:n.sites){
+        N[i, k] ~ dnegbin(prob[i, k], th)
+        prob[i, k] <- th / (th + lambda[i, k])
+        log(lambda[i, k]) <- b0 + (b1 * x1[i]) + (b2 * x2[i]) + (b3 * x3[k])
+        for (j in 1:n.surveys){
+          Y.m[i, j, k] ~ dbin(p[i,j,k], N[i,k])
+          p[i, j, k] <- exp(lp[i,j,k]) / (1 + exp(lp[i,j,k]))
+          lp[i, j, k] <- a0 + (a1 * x1.p[i]) + (a4 * x4.m[i, k])
+    }}}}
+  "
\end{CodeInput}

After specifying the \proglang{JAGS} model, we define the parameters to be monitored during the MCMC simulations, bundle numerous values and vectors from the \code{sim.data} object, and create a function for drawing random initial values for the model parameters. These steps are included in the code supplement, but are not shown here. Finally, we set the run parameters, such as the number of chains and iterations, and start the MCMC process. Run parameters were chosen such that MCMC diagnostics indicated converged chains (potential scale reduction factors < 1.05) and reasonably robust posterior distributions (effective sample sizes > 3000). Note that the recommended number of effective samples for particularly robust inference is closer to 6000 \citep{Gong_Flegal_2016}. Thus, MCMC processing times reported here could be considered optimistic estimates. The MCMC simulation is initiated with a call to \code{run.jags()}. Partial output from the simulation, related to parameter estimates, is shown below.

\begin{CodeInput}
R> out.jags.1 <- run.jags(model = jags.model.string, data = jags.data, 
+    monitor = params, n.chains = 3, inits = inits, burnin = 3000, 
+    adapt = 3000, sample = 6000, thin = 10, modules = "glm on", 
+    method = "parallel")
R> round(summary(out.jags.1), 3)[ , c(1:5, 9, 11)]
\end{CodeInput}
\begin{CodeOutput}
            Lower95 Median Upper95   Mean    SD SSeff psrf
a0            0.941  1.053   1.161  1.053 0.057  3155    1
a1           -2.380 -1.990  -1.618 -1.994 0.195  2954    1
a4            0.434  1.053   1.661  1.052 0.314  5695    1
b0            1.956  2.024   2.089  2.024 0.034  7065    1
b1            1.851  2.070   2.301  2.071 0.115  6028    1
b2           -3.142 -2.946  -2.755 -2.947 0.099 17539    1
b3            0.969  1.142   1.315  1.142 0.089 18000    1
th            2.401  2.840   3.295  2.850 0.230 18000    1
\end{CodeOutput}

Similar to the \pkg{R-INLA} analysis, median parameter estimates from the \proglang{JAGS} model were close to, and not significantly different from, the input values used to generate the data (Fig. \ref{fig:fig1}). The potential scale reduction factor for all variables was < 1.05, and the effective sample size for all variables was approximately 3000 or greater. The simulation ran in parallel on 3 virtual cores, 1 MCMC chain per core, and took approximately 2960 seconds.

%% figure 1
\begin{figure}
\includegraphics[width=\linewidth]{fig1.png}
\caption{Marginal posteriors of model parameters from \pkg{R-INLA} (dashed black lines) and \proglang{JAGS} (solid gray lines), along with Maximum Liklihood estimates (black circles) and 95\% confidence intervals (horizontal black lines) from \pkg{unmarked}.  True input values are represented by vertical black lines.}
\label{fig:fig1}
\end{figure}
%% end figure 1

\subsection[Analysis with unmarked]{Analysis with \pkg{unmarked}}
Lastly, we prepare the simulated data for the \pkg{unmarked} analysis, which involved slight modification of the \code{unmk.df} object created using the \code{sim.nmix()} function. As with the \proglang{JAGS} analysis, these steps are included in the code supplement, but are not illustrated here.

The \pkg{unmarked} analysis is run by a call to the \code{pcount()} function. The first argument in the call to \code{pcount()} is the model formula, which specifies the covariates for detection and then the covariates for abundance. This is followed by an argument identifying the unmarked data object, and the form of the mixture model, negative binomial-binomial in this case.

\begin{CodeInput}
R> out.unmk.1 <- pcount(~ 1 + x1.p + x4.m ~ 1 + x1 + x2 + x3,
+    data = unmk.data, mixture = "NB")
R> summary(out.unmk.1)
\end{CodeInput}

\begin{CodeOutput}
Abundance (log-scale):
            Estimate       SE       z     P(>|z|)
(Intercept)     2.02   0.0321    62.9    0.00e+00
x1              2.04   0.1071    19.0    1.69e-80
x2             -2.94   0.0982   -29.9    5.36e-20
x3              1.14   0.0882    13.0    2.26e-38

Detection (logit-scale):
            Estimate       SE       z     P(>|z|)
(Intercept)     1.08   0.0507   21.27   2.29e-100
x1.p           -1.90   0.1576  -12.08    1.30e-33
x4.m            1.04   0.3102    3.35    8.02e-04

Dispersion (log-scale):
Estimate          SE     z    P(>|z|)
1.05          0.0804  13.1   4.91e-39
\end{CodeOutput}

Maximum likelihood estimates for model parameters from \pkg{unmarked} were also close to, and not significantly different from, input values (Fig. \ref{fig:fig1}). Note that the dispersion estimate, after exponentiation was 2.86. The \pkg{unmarked} estimates were produced in approximately 86 seconds.

\subsection[Example I summary]{Example I summary}
Example I demonstrated basic use of \pkg{R-INLA} to analyze N-mixture models and highlighted similarities and differences between it and two other commonly used approaches. In demonstrating the use of \pkg{R-INLA}, we showed that the input data format is not too complicated, and that the formatting process can be accomplished with a few lines of code. Similarly, model specification uses a straightforward extension of the standard syntax in \proglang{R}, where the counts and covariates for $\lambda$ are specified through an \pkg{R-INLA} object included on the left side of the formula, and fixed covariates and random effects for $p$ are specified on the right side of the formula. The data format and model specification syntax of \pkg{R-INLA} is not too different from \pkg{unmarked}, whereas those of both packages are considerably different from \proglang{JAGS} and other MCMC software, such as \proglang{OpenBUGS}, \proglang{WinBUGS}, and \proglang{Stan}.

Regarding performance, \pkg{R-INLA}, \proglang{JAGS}, and \pkg{unmarked} all successfully extracted simulation input values. Fig. \ref{fig:fig1} shows marginal posterior distributions produced by \pkg{R-INLA} and \proglang{JAGS}, and estimates and 95\% confidence intervals from \pkg{unmarked}. These results derive from data from one random manifestation of the input values. Thus, we do not expect the posterior distributions for the estimates to be centered at the input values, which would be expected if the simulation was repeated many times. However, we do expect the input values to fall somewhere within the posterior distributions and 95\% confidence limits, which is what occurs here. Fig. \ref{fig:fig1} shows that, for similarly specified models, \pkg{R-INLA} (dashed black lines) and \proglang{JAGS} (solid gray lines) yielded practically identical marginal posterior distributions for model parameters. Fig. \ref{fig:fig1} also illustrates the general agreement between the credible intervals associated with \pkg{R-INLA} and \proglang{JAGS} and the confidence intervals associated with \pkg{unmarked}.

Where \pkg{R-INLA}, \proglang{JAGS}, and \pkg{unmarked} differed substantially was in computing time. In this example, \pkg{R-INLA} took 6 seconds, \proglang{JAGS} took 2960 seconds, and \pkg{unmarked} took 86 seconds to produce results. Thus, \pkg{R-INLA} was approximately 500 times faster than \proglang{JAGS} and 10 times faster than \pkg{unmarked}. This was the case despite the fact that \pkg{unmarked} produced ML estimates and the \proglang{JAGS} analysis was run in parallel with each of three MCMC chains simulated on a separate virtual computing core. If parallel computing had not been used with \proglang{JAGS}, processing the \proglang{JAGS} model would have taken approximately twice as long. If MCMC simulations were run until an effective sample size of 6000 was reached, processing time would have doubled again.

In sum, when compared to other tools, \pkg{R-INLA} is relatively easy to implement and produces accurate estimates of Bayesian posteriors very quickly. Its utility depends on the degree to which the data generating process can be captured accurately in model specification. However, as mentioned above, certain N-mixture models can not be specified using \pkg{R-INLA}. For the data in Example I, the count matrix was produced using a detection covariate that was averaged to the site-year level. This averaged covariate was subsequently specified in the model. But what happens when the site-survey-year covariate is an important component of the data generating process, and it can't be entered into the model in this form? This is the question explored in Example II.

%% example 2
\section[Example II]{Example II}
\subsection[Goals]{Goals}
In Example II, we show the consequences of not being able to specify a site-survey-year covariate under a range of conditions. We conducted a Monte Carlo experiment where, for each iteration, the count matrix for the analysis, \code{Y}, was generated with the \code{sim.nmix()} function using the site-survey-year covariate \code{x4}. The count data were then analyzed with two \proglang{JAGS} models. The first model incorporated the site-survey-year \code{x4} covariate. The second model incorporated the averaged site-survey \code{x4.m}, instead. For each iteration, we randomly varied the size of $\alpha_4$. We expected that the simpler model, with \code{x4.m}, would yield biased estimates when the magnitude of $\alpha_4$ was relatively large, and unbiased estimates when the magnitude of $\alpha_4$ was relatively small. All computing code related to Example II is given in the supplemental code file.

\subsection[Analysis with JAGS]{Analysis with \proglang{JAGS}}
Parameter values entered into \code{sim.nmix()}, other than those for $\alpha_4$, were the same as those used in Example I.  Similarly, the \proglang{JAGS} model specification, other than parts associated with $\alpha_4$, was the same as that used in Example I. Given the long processing time associated with \proglang{JAGS} models in Example I, we only ran and saved 1000 MCMC simulations (no thinning, after 500 adaptive and 100 burn-in iterations) during each of the 50 Monte Carlo runs in Example II. This number is not sufficient for drawing inference from marginal posteriors, but was sufficient for looking at qualitative patterns in posterior medians. For each of these runs, a value for $\alpha_4$ was drawn from a uniform distribution that ranged from -3 to 3. Parameter bias was represented for each model parameter as the difference between the simulation input and the posterior median estimated value. The results of the simulations are depicted in Fig. \ref{fig:fig2}.

%% figure 2
\begin{figure}
\includegraphics[width=\linewidth]{fig2.png}
\caption{Differences between posterior median parameter values and true input parameter values as a function of the magnitude of $\alpha_4$. Black circles and lines are from the model with the site-survey-year covariate, \code{x4}, and gray circles and lines are from the model with an averaged site-year covariate, \code{x4.m}. Parameter name is given in the strip across the top of each panel.}
\label{fig:fig2}
\end{figure}
%% end figure 2

\subsection[Example II summary]{Example II summary}
Even with as few as 50 Monte Carlo runs, it was apparent that biases in parameter estimates increased with the magnitude of $\alpha_4$ (Fig. \ref{fig:fig2}). When $\alpha_4$ was small, with an absolute value less than 1, the bias was negligible. When $\alpha_4$ was large, with an absolute value greater than 2, the bias was considerable (Fig. \ref{fig:fig2}). When interpreting the effect size, bear in mind that \code{x4} ranged from -0.5 to 0.5.

%% example 3
\section[Example III]{Example III}
\subsection[Goals]{Goals}
In Example III, we explore the performance of \pkg{R-INLA} using real data, a publicly available dataset of mallard duck counts in Switzerland during 2002. By employing real data, we hoped to evaluate (1) the performance of \pkg{R-INLA} using data that were not predictable by design and (2) the practical consequences of not being able to specify site-survey covariates in \pkg{R-INLA}. The dataset is available as a demonstration dataset in \pkg{unmarked}, so we compared the performance of \pkg{R-INLA} with that of \pkg{unmarked}, using the analysis settings and model structure described in \pkg{unmarked} documentation.

\subsection[Analysis with R-INLA]{Analysis with \pkg{R-INLA}}
The mallard data is provided in the \pkg{unmarked} package as a list with three components: a matrix of counts (\code{mallard.y}), a list of matrices of detection covariates (\code{mallard.obs}), and a data frame of abundance covariates (\code{mallard.site}). Data in \pkg{unmarked} are organized in structures called unmarked frames, which are viewed as a data frame when printed.

\begin{CodeInput}
R> data(mallard)
R> mallard.umf <- unmarkedFramePCount(y = mallard.y, siteCovs = 
+    mallard.site, obsCovs = mallard.obs)
R> mallard.umf[1:6, ]
\end{CodeInput}
\begin{CodeOutput}
Data frame representation of unmarkedFrame object.
   y1   y2   y3    elev length forest  ivel1  ivel2  ivel3  
1   0    0    0  -1.173  0.801 -1.156 -0.506 -0.506 -0.506 
2   0    0    0  -1.127  0.115 -0.501 -0.934 -0.991 -1.162 
3   3    2    1  -0.198 -0.479 -0.101 -1.136 -1.339 -1.610 
4   0    0    0  -0.105  0.315  0.008 -0.819 -0.927 -1.197 
5   3    0    3  -1.034 -1.102 -1.193  0.638  0.880  1.042 
6   0    0    0  -0.848  0.741  0.917 -1.329 -1.042 -0.899 
...
    date1  date2  date3
1  -1.761  0.310  1.381
2  -2.904 -1.047  0.596
3  -1.690 -0.476  1.453
4  -2.190 -0.690  1.239
5  -1.833  0.167  1.381
6  -2.619  0.167  1.381
\end{CodeOutput}

As discussed above, it is not possible to take advantage of survey-level covariates when analyzing N-mixture models with \pkg{R-INLA}.  So, before analysis with \pkg{R-INLA}, we averaged the survey-level variables, \code{ivel} and \code{date}, per site using the \code{rowMeans()} function.

\begin{CodeInput}
R> length <- mallard.site[ , "length"]
R> elev <- mallard.site[ , "elev"]
R> forest <- mallard.site[ , "forest"]
R> mean.ivel <- rowMeans(mallard.obs$ivel, na.rm = T) 
R> mean.ivel[is.na(mallard.ivel)] <- mean(mallard.ivel, na.rm = T)
R> mean.date <- rowMeans(mallard.obs$date, na.rm = T) 
R> mean.date.sq <- mean.date^2
R> mallard.inla.df <- data.frame(y1 = mallard.y[ , "y.1"], 
+    y2 = mallard.y[ , "y.2"], y3 = mallard.y[ , "y.3"], 
+    length, elev, forest, mean.ivel, mean.date, mean.date.sq)
R> round(head(mallard.inla.df), 3)
\end{CodeInput}
\begin{CodeOutput}
  y1 y2 y3 length   elev forest mean.ivel mean.date mean.date.sq
1  0  0  0  0.801 -1.173 -1.156    -0.506    -0.023        0.001
2  0  0  0  0.115 -1.127 -0.501    -1.029    -1.118        1.251
3  3  2  1 -0.479 -0.198 -0.101    -1.362    -0.238        0.056
4  0  0  0  0.315 -0.105  0.008    -0.981    -0.547        0.299
5  3  0  3 -1.102 -1.034 -1.193     0.853    -0.095        0.009
6  0  0  0  0.741 -0.848  0.917    -1.090    -0.357        0.127
\end{CodeOutput}

The data is now in a format that can be analyzed readily using \pkg{R-INLA}. The data frame has 239 sites $\times$ 1 year = 239 rows, one column for each replicate count, and one column for each detection and abundance covariate. Once in this form, it is easy to create an \code{inla.mdata()} object and run the analysis. In preparing \code{counts.and.count.covs}, we specify an intercept and effects of transect length (\code{length}), elevation (\code{elev}), and forest cover (\code{forest}) on abundance. In the \code{model} argument to \code{inla()}, we specify an intercept and effects of sampling intensity (\code{ivel}) and sampling date (\code{date}) for detection. As before, the data argument is a list that corresponds with the model formula. The family argument specifies a negative binomial-binomial mixture. The priors for intercepts and covariates are specified as vague normal distributions, and that for the overdispersion parameter as a uniform distribution. 

\begin{CodeInput}
R> counts.and.count.covs <- inla.mdata(mallard.y, 1, length, elev, forest)
R> out.inla.2 <- inla(counts.and.count.covs ~ 1 + mean.ivel +
+    mean.date + mean.date.sq, 
+    data = list(counts.and.count.covs = counts.and.count.covs, 
+      mean.ivel = mallard.inla.df$mean.ivel, mean.date = 
+      mallard.inla.df$mean.date, mean.date.sq = mallard.inla.df$mean.date.sq), 
+    family = "nmixnb", 
+    control.fixed = list(mean = 0, mean.intercept = 0, prec = 0.01, 
+      prec.intercept = 0.01),
+    control.family = list(hyper = list(theta1 = list(param = c(0, 0.01)),
+      theta2 = list(param = c(0, 0.01)), theta3 = list(param = c(0, 0.01)),
+      theta4 = list(param = c(0, 0.01)), theta5 = list(prior = "flat",
+      param = numeric()))))
R> summary(out.inla.2, digits = 3)
\end{CodeInput}

A portion of the summary for \code{out.inla.2} is shown below. Note that posterior summaries described in the fixed effects section pertain to the intercept and covariates of $p$. In the hyperparameters section, \code{beta[1]}, \code{beta[2]}, \code{beta[3]}, and \code{beta[4]} identify posterior summaries for the $\lambda$ intercept, and transect length, elevation, and forest cover effects.

\begin{CodeOutput}
Fixed effects:
               mean    sd 0.025quant  0.5quant  0.975quant
(Intercept)  -0.397 0.383     -1.170    -0.389       0.335
mean.ivel     0.039 0.212     -0.378     0.039       0.455
mean.date    -1.044 0.433     -1.923    -1.036      -0.195
mean.date.sq -0.318 0.304     -0.962    -0.301       0.233

The model has no random effects.

Model hyperparameters:
             mean     sd  0.025quant  0.5quant  0.975quant
beta[1]    -1.412  0.296      -1.966    -1.424      -0.801 
beta[2]    -0.290  0.190      -0.664    -0.291       0.086
beta[3]    -0.998  0.318      -1.595    -1.011      -0.341
beta[4]    -0.771  0.203      -1.178    -0.767      -0.382
overdisp    1.228  0.264       0.799     1.194       1.837
\end{CodeOutput}

\subsection[Analysis with unmarked]{Analysis with \pkg{unmarked}}
The \pkg{unmarked} frame, \code{mallard.umf}, created above, can be used directly by the \code{pcount()} function in \pkg{unmarked}. The data and model structure described in the \code{pcount()} function below is similar to that used above in the \pkg{R-INLA} analysis, except for one key difference: here, \code{ivel} and \code{date} are site-survey level variables instead of the site-level means used for the \pkg{R-INLA} analysis.

\begin{CodeInput}
R> out.unmk.2 <- pcount(~ ivel+ date + I(date^2) ~ length + elev + forest,
+    mixture = "NB", mallard.umf)
R> summary(out.unmk.2)
\end{CodeInput}
\begin{CodeOutput}
Abundance (log-scale):
             Estimate         SE           z     P(>|z|)
(Intercept)    -1.786      0.281      -6.350    2.15e-10
length         -0.186      0.214      -0.868    3.86e-01
elev           -1.372      0.293      -4.690    2.73e-06
forest         -0.685      0.216      -3.166    1.54e-03

Detection (logit-scale):
             Estimate         SE           z     P(>|z|)
(Intercept)    -0.028      0.285      -0.099       0.921
ivel            0.174      0.227       0.766       0.444
date           -0.313      0.147      -2.132       0.033
I(date^2)      -0.005      0.081      -0.059       0.953

Dispersion (log-scale):
Estimate          SE       z    P(>|z|)
-0.695         0.364   -1.91      0.056
\end{CodeOutput}

\subsection[Example III summary]{Example III summary}
Comparing the results, we see that the 95\% credible intervals for parameter estimates from the \pkg{R-INLA} analysis overlapped broadly with the 95\% confidence intervals from the \pkg{unmarked} analysis, so parameter estimates were not significantly different from one another (Fig. \ref{fig:fig3}). Regardless of technique, the same set of parameters had estimates significantly different from zero (Fig. \ref{fig:fig3}), and significant effects were of the same magnitude and direction in both analyses. Using both techniques, detection decreased as the season progressed, and abundance decreased with increasing forest cover and elevation. Parameter estimates and biological conclusions were similar despite the fact that site-survey detection covariates were used for \pkg{unmarked} and site-averaged detection covariates were used for \pkg{R-INLA}. Note that \pkg{unmarked} estimated moderate effects of detection covariates which, according to the results in Example II, would indicate that parameter estimates from the \pkg{R-INLA} analysis were not substantially biased. These conclusions may have been different given a different dataset, where detection covariates had very strong effects.

%% figure 3
\begin{figure}
\includegraphics[width=\linewidth]{fig3.png}
\caption{Parameter estimates and 95\% confidence intervals from \pkg{unmarked} (gray circles and lines) and posterior medians and 95\% credible intervals from \pkg{R-INLA} (black lines) from an N-mixture model analysis of mallard duck abundance. Model parameters are identified by their associated variable names listed on the vertical axis.  The \pkg{unmarked} model included site-survey covariates for survey intensity and survey date, while the \pkg{R-INLA} model included site-averaged versions. A value of zero (no effect) is depicted by the vertical dashed gray line.}
\label{fig:fig3}
\end{figure}
%% end figure 3

%% discussion
\section[Discussion]{Discussion}
The purpose of this work was to detail the use of the \pkg{R-INLA} package \citep{Rue_Riebler_Sorbye_Illian_Simpson_Lindgren_2017} to analyze N-mixture models and to compare analyses using \pkg{R-INLA} to two other common approaches: \proglang{JAGS} \citep{plummer2003jags,Lunn_Jackson_Best_Thomas_Spiegelhalter_2012}, via the \pkg{runjags} package \citep{Denwood_2016}, which employs MCMC methods and allows Bayesian inference, and the \pkg{unmarked} package \citep{Fiske_Chandler_2011}, which uses maximum likelihood and allows frequentist inference. While we selected \proglang{JAGS} as the representative MCMC approach, we expect that our conclusions would be qualitatively similar for other MCMC software, such as \proglang{OpenBUGS}, \proglang{WinBUGS}, or \proglang{Stan}. We are not aware of other commonly-used software for analyzing N-mixture models in a maximum likelihood framework, besides \pkg{unmarked}.

Comparisons showed that \pkg{R-INLA} can be a complementary tool in the wildlife biologist's analytical tool kit. Strengths of \pkg{R-INLA} include Bayesian inference, based on highly accurate approximations of posterior distributions, which were derived roughly 500 times faster than MCMC methods, where models are specified using a syntax that should be familiar to R users, and where data are formatted in a straightforward way with relatively few lines of code. The straightforward model syntax and data format could help lower  barriers to adoption of N-mixture models for biologists who are not committed to learning  \proglang{BUGS} or \proglang{Stan} syntax. The substantial decrease in computation time should facilitate use of a wider variety of model and variable selection techniques (e.g., cross validation and model averaging), ones that are not commonly used in an MCMC context due to practical issues related to computing time \citep{Kery_Schaub_2011}.

Limitations of \pkg{R-INLA} are mainly related to the more restricted set of N-mixture models that can be specified. Of the approaches described here, ones that use MCMC allow users ultimate flexibility in specifying models. For example, with \proglang{JAGS}, site-survey-year covariates for detection are possible, multiple types of mixed distributions are available \citep{Joseph_Elkin_Martin_Possingham_2009,Martin_Royle_Mackenzie_Edwards_Kery_Gardner_2011}, and a variety of random effects can be specified for both $\lambda$ and $p$ \citep{Kery_Schaub_2011}. In comparison, the current version of \pkg{R-INLA} does not handle site-survey covariates, employs only Poisson-binomial and negative binomial-binomial mixtures, and handles random effects for $p$ only. A practical consequence of the random effects limitation is that, while site and site-year posteriors for lambda can be estimated using \pkg{R-INLA}, site and site-year posteriors for $N$ are not currently available (see Appendix). In cases where site-survey covariates are particularly important, and not otherwise controlled by sampling design, where different mixed distributions are required, or where random effects associated with $\lambda$ are needed, an MCMC approach appears to be most appropriate (Fig. \ref{fig:fig2}).

When compared to \pkg{unmarked}, the \pkg{R-INLA} approach is similar in regards to familiar model syntax and data format. The approaches are also similar in that both yield results much faster than MCMC, enabling a richer set of options in terms of model and variable selection. The two approaches differ in that \pkg{R-INLA} is approximately 10 times faster than \pkg{unmarked}, likely due to the different approach used to compute model likelihoods (see Appendix). They also differ in that \pkg{unmarked} can accommodate site-survey covariates, whereas \pkg{R-INLA} does not, and that \pkg{R-INLA} can accomodate random effects for $p$, whereas \pkg{unmarked} does not. In cases where both computing speed and specification of site-survey covariates are critical, \pkg{unnmarked} appears to an appropriate tool.

In conclusion, \pkg{R-INLA}, \proglang{JAGS} (and \proglang{WinBUGS}, \proglang{OpenBUGS}, and \proglang{Stan}), and \pkg{unmarked} all allow users to analyze N-mixture models for estimating wildlife abundance while accounting for imperfect detection. Each method has its strengths and limitations. \pkg{R-INLA} appears to be an attractive option when survey-level covariates are not essential, familiar model syntax and data format are desired, Bayesian inference is preferred, and fast computing time is required.

%% acknowledgements
\section*{Acknowledgments}
We thank C. Burkhalter, T. Onkelinx, U. Halekoh, E. Pebesma, and an anonymous reviewer for commenting on previous drafts of this manuscript.

%% references
\bibliography{bibliog}

%% appendix
\appendix
\section{Recursive computations of the 'nmix' likelihood}
The likelihood for the simplest case is
\begin{displaymath}
	\text{Prob}(y) = \sum_{N = y}^{\infty}
	\text{Pois}(N ; \lambda) \;\times\; \text{Bin}(y;  N, p)
\end{displaymath}
where $\text{Pois}(N; \lambda)$ is the density for the Poisson distribution with mean $\lambda$, $\lambda^{N}\exp(-\lambda)/N!$, and $\text{Bin}(y; N, p)$ is the density for the binomial distribution with $N$ trials and probability $p$, ${N \choose y} p^{y}(1-p)^{N-p}$. Although the likelihood can be computed directly when replacing the infinite limit with a finite value, we will demonstrate here that we can easily evaluate it using a recursive algorithm that is both faster and more numerical stable. The same idea is also applicable to the negative binomial case, and the case where we have replicated observations of the same $N$. We leave it to the reader to derive these straight forward extensions.

The key observation is that both the Poisson and the binomial distribution can be evaluated recursively in $N$,
\begin{displaymath}
	\text{Pois}(N; \lambda) = \text{Pois}(N-1; \lambda) \frac{\lambda}{N}
\end{displaymath}
and
\begin{displaymath}
	\text{Bin}(y; N, p) = \text{Bin}(y; N-1, p) \frac{N}{N-y}(1-p),
\end{displaymath}
and then also for the Poisson-binomial product
\begin{displaymath}
	\text{Pois}(N ; \lambda) \; \text{Bin}(y; N, p)
	=
	\text{Pois}(N-1; \lambda) \; \text{Bin}(y; N-1, p)
	\frac{\lambda}{N-y}(1-p).
\end{displaymath}
If we define $f_i = \lambda(1-p)/i$ for $i=1, 2, \ldots$, we can make use of this recursive form to express the likelihood with a finite upper limit as
\begin{eqnarray}
	\text{Prob}(y) &=& \sum_{N = y}^{N_{\text{max}}}
	\text{Pois}(N ; \lambda)\;
	\text{Bin}(y; N, p) \nonumber\\
	&=& \text{Pois}(y; \lambda)\; \text{Bin}(y; y, p)
	\Big\{ 1 + f_1 + f_1f_2 +
	\ldots
	+f_1\cdots f_{N_\text{max}}
	\Big\} \nonumber\\
	&=& \text{Pois}(y; \lambda)\; \text{Bin}(y; y, p)
	\Big\{ 1 + f_1(1+f_2(1+f_3(1+ \dots)))\Big\}\nonumber
\end{eqnarray}
The log-likelihood can then be evaluated using the following simple \proglang{R} code.
\begin{CodeInput}
R> fac <- 1; ff <- lambda * (1-p)
R> for(i in (N.max - y):1) fac <- 1 + fac * ff / i
R> log.L <- dpois(y, lambda, log = TRUE) +
+    dbinom(y, y, p, log = TRUE) + log(fac)
\end{CodeInput}
Since this evaluation is recursive in decreasing $N$, we have to choose the upper limit $N_\text{max}$ in advance, for example as an integer larger than $y$ so that $\frac{\lambda (1-p)}{N_\text{max}-y}$ is small. Note that we are computing \code{fac} starting with the smallest contributions, which are more numerically stable.

\section{Posterior probability for $N$}
Currently, it is not possible to extract posteriors for $N$ when analyzing N-mixture models using \pkg{R-INLA}. This functionality, which would utilize output from the \code{inla.posterior.sample()} function, could be available in future versions based on the following logic. Assume the Poisson for $N$, such that $\textnormal{Prob} (N | \lambda) = p_0(N; \lambda)$, and

$$\textnormal{Prob} (y_1, ..., y_m|N) = \sum \bigg[ \prod_{i=1}^{m} \textnormal{Bin} (y_i; N, p) \bigg] p_0(N; \lambda),$$

where $y_1, ..., y_m = \Upsilon$, and $N \geq \textnormal{max} (y_1, ..., y_m)$. If we have samples from the posterior of $(p, \lambda) | \Upsilon$, we can compute the posterior marginal of $N | \Upsilon$ as follows. If $(p, \lambda)$ is fixed, then

$$\textnormal{Prob} (N | \Upsilon) \propto \bigg[ \prod_{i=1}^{m} \textnormal{Bin} (y_i; N, p) \bigg] p_0 (N; \lambda),$$

and this expression is evaluated for $N = \textnormal{max}(y_1, ..., y_m)$, ..., and renormalized. We can integrate out $(p, \lambda) | \Upsilon$ using samples from the posteriors, as

$$\textnormal{Prob} (N | \Upsilon) = \frac{1}{M}  \sum_{j=1}^{M} \frac{1}{Z(p_j,\lambda_j)}  \bigg[ \prod_{i=1}^{M} \textnormal{Bin} (y_i; N, p_j) \bigg] p_0 (N; \lambda_j),$$

for $M$ samples $(p_1, \lambda_1), ..., (p_M, \lambda_M)$ from the posterior of $(p, \lambda) | \Upsilon$. That is, we average the probability for each $N$, renormalize, and normalize for each sample by computing $Z (p, \lambda)$. 
	
	
\end{document}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
